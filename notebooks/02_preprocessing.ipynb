{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 02_preprocessing.ipynb\n",
                "\n",
                "**Purpose**: Load raw NetCDF files, normalize data, create sliding window sequences, and save Train/Val/Test splits.\n",
                "\n",
                "**Fixed (v2)**: \n",
                "- Handles both `valid_time` and `time` coordinates\n",
                "- Supports both `data_0*.nc` and `era5land_*.nc` file patterns\n",
                "- Robust error handling for CDS API changes"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 1: Mount Drive & Install Dependencies\n",
                "from google.colab import drive\n",
                "drive.mount('/content/drive')\n",
                "\n",
                "PROJECT_ROOT = '/content/drive/MyDrive/WeatherPaper'\n",
                "\n",
                "!pip install xarray netCDF4 pyyaml numpy"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 2: Imports and Configuration\n",
                "import os\n",
                "import glob\n",
                "import yaml\n",
                "import numpy as np\n",
                "import xarray as xr\n",
                "import pandas as pd\n",
                "\n",
                "# Load Config\n",
                "config_path = os.path.join(PROJECT_ROOT, 'config/project_scope.yaml')\n",
                "if os.path.exists(config_path):\n",
                "    with open(config_path, 'r') as f:\n",
                "        scope_config = yaml.safe_load(f)\n",
                "else:\n",
                "    # Default config if not found\n",
                "    scope_config = {\n",
                "        'time_split': {\n",
                "            'train_years': [2015, 2016, 2017, 2018, 2019, 2020, 2021],\n",
                "            'val_years': [2022, 2023],\n",
                "            'test_years': [2024, 2025]\n",
                "        }\n",
                "    }\n",
                "    print(\"Using default config.\")\n",
                "\n",
                "print(\"Config Loaded.\")\n",
                "print(f\"Train Years: {scope_config['time_split']['train_years']}\")\n",
                "print(f\"Val Years: {scope_config['time_split']['val_years']}\")\n",
                "print(f\"Test Years: {scope_config['time_split']['test_years']}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load Dataset (Robust Approach)\n",
                "\n",
                "We load ALL files directly. The `preprocess` function handles:\n",
                "- `valid_time` â†’ `time` coordinate renaming (CDS API v2 change)\n",
                "- `expver` dimension flattening\n",
                "- Extra coordinate cleanup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 3: Define Preprocessing Function (FIXED FOR CDS API v2)\n",
                "def preprocess_era5(ds):\n",
                "    \"\"\"Standardize each file before combining.\n",
                "    \n",
                "    Handles:\n",
                "    - valid_time -> time renaming (CDS API v2)\n",
                "    - expver dimension flattening\n",
                "    - Cleanup of extra coordinates\n",
                "    \"\"\"\n",
                "    \n",
                "    # 1. Rename valid_time -> time (CDS API v2 uses valid_time)\n",
                "    if 'valid_time' in ds.coords and 'time' not in ds.coords:\n",
                "        ds = ds.rename({'valid_time': 'time'})\n",
                "        print(f\"  Renamed valid_time -> time\")\n",
                "    \n",
                "    # 2. Handle expver dimension (ERA5 vs ERA5T mixing)\n",
                "    if 'expver' in ds.dims:\n",
                "        # Flatten expver: prefer ERA5 (1), fill gaps with ERA5T (5)\n",
                "        try:\n",
                "            # Try string keys first (common in newer CDS downloads)\n",
                "            ds_1 = ds.sel(expver='0001')\n",
                "            ds_5 = ds.sel(expver='0005')\n",
                "            ds = ds_1.combine_first(ds_5)\n",
                "        except (KeyError, ValueError):\n",
                "            try:\n",
                "                # Try integer keys\n",
                "                ds_1 = ds.sel(expver=1)\n",
                "                ds_5 = ds.sel(expver=5)\n",
                "                ds = ds_1.combine_first(ds_5)\n",
                "            except (KeyError, ValueError):\n",
                "                # Fallback: just take the first index\n",
                "                ds = ds.isel(expver=0, drop=True)\n",
                "    elif 'expver' in ds.coords:\n",
                "        # It's a coordinate but not a dimension - drop it\n",
                "        ds = ds.drop_vars('expver', errors='ignore')\n",
                "    \n",
                "    # 3. Drop 'number' coordinate if present (ensemble member indicator)\n",
                "    if 'number' in ds.coords:\n",
                "        ds = ds.drop_vars('number', errors='ignore')\n",
                "    \n",
                "    # 4. Ensure float32 for memory efficiency\n",
                "    for var in ds.data_vars:\n",
                "        if ds[var].dtype == 'float64':\n",
                "            ds[var] = ds[var].astype('float32')\n",
                "        \n",
                "    return ds"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 4: Find and Load All Files\n",
                "raw_dir = os.path.join(PROJECT_ROOT, 'data/raw')\n",
                "\n",
                "# Support both file naming patterns\n",
                "all_files = sorted(glob.glob(os.path.join(raw_dir, \"era5land_*.nc\")))\n",
                "if not all_files:\n",
                "    # Try alternative pattern (data_0*.nc from direct downloads)\n",
                "    all_files = sorted(glob.glob(os.path.join(raw_dir, \"data_0*.nc\")))\n",
                "if not all_files:\n",
                "    # Try any NetCDF file\n",
                "    all_files = sorted(glob.glob(os.path.join(raw_dir, \"*.nc\")))\n",
                "\n",
                "print(f\"Found {len(all_files)} NetCDF files.\")\n",
                "if all_files:\n",
                "    print(f\"Sample: {os.path.basename(all_files[0])}\")\n",
                "\n",
                "if not all_files:\n",
                "    raise FileNotFoundError(f\"No NetCDF files found in {raw_dir}\")\n",
                "\n",
                "# Quick validation of first file\n",
                "print(\"\\nValidating first file structure...\")\n",
                "test_ds = xr.open_dataset(all_files[0])\n",
                "print(f\"  Coordinates: {list(test_ds.coords)}\")\n",
                "print(f\"  Variables: {list(test_ds.data_vars)}\")\n",
                "print(f\"  Dimensions: {dict(test_ds.dims)}\")\n",
                "test_ds.close()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 5: Load All Files with Preprocessing\n",
                "print(\"Loading dataset (this may take a few minutes)...\")\n",
                "\n",
                "# Load with netcdf4 engine\n",
                "# parallel=False to prevent HDF5 locking issues on Drive\n",
                "ds = xr.open_mfdataset(\n",
                "    all_files,\n",
                "    combine='by_coords',\n",
                "    engine='netcdf4',\n",
                "    preprocess=preprocess_era5,\n",
                "    parallel=False,  # Critical: prevents HDF5 race conditions\n",
                "    lock=False       # Also helps with Drive access\n",
                ")\n",
                "\n",
                "# Sort by time to ensure chronological order\n",
                "ds = ds.sortby('time')\n",
                "\n",
                "print(f\"\\n=== DATASET LOADED SUCCESSFULLY ===\")\n",
                "print(f\"Time range: {str(ds.time.values[0])[:19]} to {str(ds.time.values[-1])[:19]}\")\n",
                "print(f\"Variables: {list(ds.data_vars)}\")\n",
                "print(f\"Dimensions: {dict(ds.dims)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 6: Enforce Hourly Continuity\n",
                "print(\"Resampling to hourly frequency (filling gaps with NaN)...\")\n",
                "ds = ds.resample(time='1h').asfreq()\n",
                "print(f\"Total hourly timesteps: {len(ds.time)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Normalization (Train Stats Only)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 7: Load into Memory and Normalize\n",
                "train_years = scope_config['time_split']['train_years']\n",
                "val_years = scope_config['time_split']['val_years']\n",
                "test_years = scope_config['time_split']['test_years']\n",
                "\n",
                "# Use variables in consistent order\n",
                "available_vars = list(ds.data_vars)\n",
                "ordered_vars = [v for v in ['tp', 't2m', 'msl'] if v in available_vars]\n",
                "if not ordered_vars:\n",
                "    ordered_vars = available_vars\n",
                "\n",
                "print(f\"Processing variables: {ordered_vars}\")\n",
                "\n",
                "# Convert to numpy array: (time, lat, lon, channel)\n",
                "print(\"Loading into memory...\")\n",
                "data_xr = ds[ordered_vars].to_array(dim='channel').transpose('time', 'latitude', 'longitude', 'channel')\n",
                "data_np = data_xr.values.astype(np.float32)\n",
                "times = ds.time.values\n",
                "\n",
                "print(f\"Data shape: {data_np.shape}\")\n",
                "\n",
                "# Create year-based masks\n",
                "years = pd.to_datetime(times).year\n",
                "train_mask = np.isin(years, train_years)\n",
                "val_mask = np.isin(years, val_years)\n",
                "test_mask = np.isin(years, test_years)\n",
                "\n",
                "print(f\"Train samples: {train_mask.sum()}, Val samples: {val_mask.sum()}, Test samples: {test_mask.sum()}\")\n",
                "\n",
                "# Compute normalization stats from TRAINING data only (prevent leakage)\n",
                "train_data = data_np[train_mask]\n",
                "mean = np.nanmean(train_data, axis=(0, 1, 2), keepdims=True)\n",
                "std = np.nanstd(train_data, axis=(0, 1, 2), keepdims=True)\n",
                "std = np.where(std < 1e-6, 1.0, std)  # Prevent division by zero\n",
                "\n",
                "print(f\"Mean: {mean.flatten()}\")\n",
                "print(f\"Std: {std.flatten()}\")\n",
                "\n",
                "# Normalize entire dataset\n",
                "data_norm = (data_np - mean) / std\n",
                "print(\"Normalization complete.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Create Sequences"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 8: Sequence Generation with NaN Filtering\n",
                "T_IN = 24   # 24 hours of input (past)\n",
                "T_OUT = 6   # 6 hours of output (future)\n",
                "\n",
                "def create_sequences(data, timestamps, year_mask, t_in, t_out):\n",
                "    \"\"\"Create input/output sequences, filtering out any with NaN values.\"\"\"\n",
                "    X_list, Y_list, T_list = [], [], []\n",
                "    \n",
                "    # Get indices where year mask is true\n",
                "    valid_idx = np.where(year_mask)[0]\n",
                "    # Ensure we have enough context before and after\n",
                "    valid_idx = valid_idx[(valid_idx >= t_in) & (valid_idx < len(data) - t_out)]\n",
                "    \n",
                "    nan_skip_count = 0\n",
                "    \n",
                "    for i in valid_idx:\n",
                "        x_seq = data[i - t_in:i]       # Past 24 hours\n",
                "        y_seq = data[i:i + t_out]      # Next 6 hours\n",
                "        \n",
                "        # Skip if any NaN values (gap in data)\n",
                "        if np.isnan(x_seq).any() or np.isnan(y_seq).any():\n",
                "            nan_skip_count += 1\n",
                "            continue\n",
                "        \n",
                "        X_list.append(x_seq)\n",
                "        Y_list.append(y_seq)\n",
                "        T_list.append(timestamps[i])\n",
                "    \n",
                "    print(f\"  Created {len(X_list)} sequences, skipped {nan_skip_count} due to NaN.\")\n",
                "    \n",
                "    if not X_list:\n",
                "        return np.array([]), np.array([]), np.array([])\n",
                "    \n",
                "    return np.array(X_list, dtype='float32'), np.array(Y_list, dtype='float32'), np.array(T_list)\n",
                "\n",
                "print(\"Generating sequences...\")\n",
                "print(\"Train:\")\n",
                "X_train, Y_train, T_train = create_sequences(data_norm, times, train_mask, T_IN, T_OUT)\n",
                "print(\"Validation:\")\n",
                "X_val, Y_val, T_val = create_sequences(data_norm, times, val_mask, T_IN, T_OUT)\n",
                "print(\"Test:\")\n",
                "X_test, Y_test, T_test = create_sequences(data_norm, times, test_mask, T_IN, T_OUT)\n",
                "\n",
                "print(f\"\\nFinal shapes:\")\n",
                "print(f\"  Train: X={X_train.shape}, Y={Y_train.shape}\")\n",
                "print(f\"  Val:   X={X_val.shape}, Y={Y_val.shape}\")\n",
                "print(f\"  Test:  X={X_test.shape}, Y={Y_test.shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 9: Save Processed Data\n",
                "processed_dir = os.path.join(PROJECT_ROOT, 'data/processed')\n",
                "os.makedirs(processed_dir, exist_ok=True)\n",
                "\n",
                "# Save splits\n",
                "np.savez_compressed(os.path.join(processed_dir, 'train.npz'), x=X_train, y=Y_train, time=T_train)\n",
                "np.savez_compressed(os.path.join(processed_dir, 'val.npz'), x=X_val, y=Y_val, time=T_val)\n",
                "np.savez_compressed(os.path.join(processed_dir, 'test.npz'), x=X_test, y=Y_test, time=T_test)\n",
                "\n",
                "# Save normalization stats for inference\n",
                "np.savez_compressed(os.path.join(processed_dir, 'stats.npz'), mean=mean, std=std, variables=ordered_vars)\n",
                "\n",
                "print(f\"\\n=== PREPROCESSING COMPLETE ===\")\n",
                "print(f\"Saved to: {processed_dir}\")\n",
                "print(f\"  - train.npz: {X_train.shape[0]} samples\")\n",
                "print(f\"  - val.npz:   {X_val.shape[0]} samples\")\n",
                "print(f\"  - test.npz:  {X_test.shape[0]} samples\")\n",
                "print(f\"  - stats.npz: normalization parameters\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}