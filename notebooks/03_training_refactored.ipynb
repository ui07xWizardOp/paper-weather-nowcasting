{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 03_training_refactored.ipynb - Weather Nowcasting with ConvLSTM\n",
    "# Implements Lead Architect Fixes: Data Integrity, Speed, Robust Persistence\n",
    "# ============================================================================\n",
    "\n",
    "import os, gc, glob, time, logging, shutil, re\n",
    "from typing import Tuple, List, Optional\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import psutil\n",
    "\n",
    "# ============================================================================\n",
    "# 1. REPRODUCIBILITY & CONFIGURATION\n",
    "# ============================================================================\n",
    "RANDOM_SEED = 42\n",
    "def seed_everything(seed=42):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False # Strict reproducibility\n",
    "\n",
    "seed_everything(RANDOM_SEED)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "print('\u2705 Reproducibility seeds set.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # Paths\n",
    "    WORK_DIR = Path('/content/weather_nowcasting')\n",
    "    DATA_DIR = WORK_DIR / 'Dataset'\n",
    "    DATA_2025_DIR = WORK_DIR / '2025_data'\n",
    "    BATCHED_DIR = WORK_DIR / 'data' / 'batched'\n",
    "    \n",
    "    # Local Checkpoints (High Speed I/O)\n",
    "    LOCAL_CKPT_DIR = WORK_DIR / 'checkpoints'\n",
    "    \n",
    "    # Drive Persistence\n",
    "    DRIVE_MOUNT = '/content/drive'\n",
    "    DRIVE_ROOT = Path('/content/drive/MyDrive/WeatherPaper')\n",
    "    DRIVE_CKPT_DIR = DRIVE_ROOT / 'checkpoints'\n",
    "    \n",
    "    # Data\n",
    "    T_IN = 24\n",
    "    T_OUT = 6\n",
    "    STRIDE = 12\n",
    "    VARIABLES = ['tp', 't2m']\n",
    "    \n",
    "    # Model\n",
    "    IN_CHANNELS = 2\n",
    "    HIDDEN_DIM = 128\n",
    "    OUT_CHANNELS = 2\n",
    "    N_LAYERS = 4\n",
    "    \n",
    "    # Training\n",
    "    BATCH_SIZE = 32\n",
    "    NUM_WORKERS = 2\n",
    "    LEARNING_RATE = 1e-4\n",
    "    WEIGHT_DECAY = 1e-5\n",
    "    NUM_EPOCHS = 100\n",
    "    PATIENCE = 20\n",
    "    MAX_GRAD_NORM = 1.0\n",
    "    PRECIP_WEIGHT = 2.0\n",
    "    \n",
    "CFG = Config()\n",
    "\n",
    "# Create Directories\n",
    "for p in [CFG.WORK_DIR, CFG.DATA_DIR, CFG.DATA_2025_DIR, CFG.BATCHED_DIR, CFG.LOCAL_CKPT_DIR]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 2. DEVICE & DRIVE SETUP\n",
    "# ============================================================================\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device} | GPU: {torch.cuda.get_device_name(0) if device.type=='cuda' else 'N/A'}\")\n",
    "\n",
    "# --- DRIVE MOUNT (ROBUST) ---\n",
    "# We define the save directory. We default to local (fast).\n",
    "# We will copy to Drive at the end or asynchronously.\n",
    "SAVE_DIR = CFG.LOCAL_CKPT_DIR \n",
    "DRIVE_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    print(\"Mounting Google Drive...\")\n",
    "    drive.mount(CFG.DRIVE_MOUNT, force_remount=True)\n",
    "    \n",
    "    # Verify and Create Drive Folders\n",
    "    if os.path.exists(CFG.DRIVE_MOUNT):\n",
    "        CFG.DRIVE_CKPT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "        DRIVE_AVAILABLE = True\n",
    "        print(f\"\u2705 Drive Mounted. Backups will sync to: {CFG.DRIVE_CKPT_DIR}\")\n",
    "    else:\n",
    "        print(\"\u26a0\ufe0f Drive mount point not found.\")\n",
    "except ImportError:\n",
    "    print(\"\u26a0\ufe0f Not in Colab. Using local checkpoints.\")\n",
    "except Exception as e:\n",
    "    print(f\"\u26a0\ufe0f Drive Mount Failed: {e}. Training locally (checkpoints will be lost if runtime disconnects).\")\n",
    "\n",
    "print(f\"\ud83d\udcbe Primary Save Directory (Local): {SAVE_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 3. DOWNLOAD DATA (GITHUB)\n",
    "# ============================================================================\n",
    "import requests, zipfile, io\n",
    "\n",
    "def download_github_repo():\n",
    "    url = 'https://github.com/ui07xWizardOp/paper-weather-nowcasting/archive/refs/heads/main.zip'\n",
    "    print(f'Downloading repository: {url}')\n",
    "    r = requests.get(url, stream=True)\n",
    "    if r.status_code != 200: raise Exception(f\"Download failed: {r.status_code}\")\n",
    "    \n",
    "    z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "    print('Extracting NetCDF files...')\n",
    "    for file in tqdm(z.namelist(), desc='Extracting'):\n",
    "        if file.endswith('.nc'):\n",
    "            if '/Dataset/' in file:\n",
    "                with open(CFG.DATA_DIR / os.path.basename(file), 'wb') as f: f.write(z.read(file))\n",
    "            elif '/2025 data/' in file:\n",
    "                with open(CFG.DATA_2025_DIR / os.path.basename(file), 'wb') as f: f.write(z.read(file))\n",
    "    print('\u2705 Download complete.')\n",
    "\n",
    "if not list(CFG.DATA_DIR.glob('*.nc')) or not list(CFG.DATA_2025_DIR.glob('*.nc')):\n",
    "    download_github_repo()\n",
    "else:\n",
    "    print('\u2705 Dataset exists.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 4. PREPROCESSING (STREAMING) - FIXED STAT LOADING\n",
    "# ============================================================================\n",
    "import xarray as xr\n",
    "\n",
    "# --- HELPERS ---\n",
    "def load_single_file(filepath):\n",
    "    for engine in ['netcdf4', 'h5netcdf', 'scipy', None]:\n",
    "        try:\n",
    "            ds = xr.open_dataset(filepath, engine=engine) if engine else xr.open_dataset(filepath)\n",
    "            break\n",
    "        except: continue\n",
    "    else: return None\n",
    "    \n",
    "    # Coordinate Standardization\n",
    "    if 'valid_time' in ds.coords: ds = ds.rename({'valid_time': 'time'})\n",
    "    if 'expver' in ds.dims: ds = ds.isel(expver=0, drop=True)\n",
    "    if 'expver' in ds.coords: ds = ds.drop_vars('expver', errors='ignore')\n",
    "    if 'number' in ds.coords: ds = ds.drop_vars('number', errors='ignore')\n",
    "    return ds\n",
    "\n",
    "# --- MAIN PREPROCESSING LOGIC ---\n",
    "stats_path = CFG.BATCHED_DIR / 'stats.npz'\n",
    "reprocess = not (CFG.BATCHED_DIR / 'train').exists() or not list((CFG.BATCHED_DIR / 'train').glob('X_batch_*.npy'))\n",
    "\n",
    "if reprocess:\n",
    "    print('\ud83d\ude80 Starting Preprocessing...')\n",
    "    for split in ['train', 'val', 'test']: (CFG.BATCHED_DIR / split).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    files = sorted(list(CFG.DATA_DIR.glob('*.nc')) + list(CFG.DATA_2025_DIR.glob('*.nc')))\n",
    "    \n",
    "    # PASS 1: Stats\n",
    "    print('Pass 1: Computing Statistics...')\n",
    "    train_values = []\n",
    "    TRAIN_YEARS = range(2015, 2022)\n",
    "    \n",
    "    for f in tqdm(files, desc='Stats'):\n",
    "        ds = load_single_file(f)\n",
    "        if ds is None: continue\n",
    "        try:\n",
    "            year = pd.to_datetime(ds.time.values[0]).year\n",
    "            if year in TRAIN_YEARS:\n",
    "                # Extract and Transform\n",
    "                tp = np.log1p(np.maximum(ds['tp'].values if 'tp' in ds else ds['total_precipitation'].values, 0))\n",
    "                t2m = ds['t2m'].values if 't2m' in ds else ds['2m_temperature'].values\n",
    "                data = np.stack([tp, t2m], axis=-1)\n",
    "                train_values.append(data[::24]) # Subsample\n",
    "        except Exception as e: print(f\"Skip {f.name}: {e}\")\n",
    "        finally: ds.close()\n",
    "            \n",
    "    train_sample = np.concatenate(train_values, axis=0)\n",
    "    mean = np.nanmean(train_sample, axis=(0, 1, 2))\n",
    "    std = np.nanstd(train_sample, axis=(0, 1, 2))\n",
    "    std[std < 1e-6] = 1.0\n",
    "    np.savez(CFG.BATCHED_DIR / 'stats.npz', mean=mean, std=std)\n",
    "    print(f'Stats \u2014 Mean: {mean}, Std: {std}')\n",
    "    del train_values, train_sample; gc.collect()\n",
    "    \n",
    "    # PASS 2: Sequences\n",
    "    print('Pass 2: Creating Sequences...')\n",
    "    buffers = {'train': ([], []), 'val': ([], []), 'test': ([], [])}\n",
    "    counts = {'train': 0, 'val': 0, 'test': 0}\n",
    "    \n",
    "    def flush(split):\n",
    "        xs, ys = buffers[split]\n",
    "        if len(xs) >= 500:\n",
    "            X = np.stack(xs[:500])\n",
    "            Y = np.stack(ys[:500])\n",
    "            np.save(CFG.BATCHED_DIR / split / f'X_batch_{counts[split]:04d}.npy', X)\n",
    "            np.save(CFG.BATCHED_DIR / split / f'Y_batch_{counts[split]:04d}.npy', Y)\n",
    "            counts[split] += 1\n",
    "            buffers[split] = (xs[500:], ys[500:])\n",
    "\n",
    "    for f in tqdm(files, desc='Processing'):\n",
    "        ds = load_single_file(f)\n",
    "        if ds is None: continue\n",
    "        \n",
    "        year = pd.to_datetime(ds.time.values[0]).year\n",
    "        # Extract and Transform\n",
    "        tp = np.log1p(np.maximum(ds['tp'].values if 'tp' in ds else ds['total_precipitation'].values, 0))\n",
    "        t2m = ds['t2m'].values if 't2m' in ds else ds['2m_temperature'].values\n",
    "        data = np.stack([tp, t2m], axis=-1)\n",
    "        \n",
    "        # Normalize\n",
    "        data = (data - mean) / std\n",
    "        data = np.nan_to_num(data, nan=0.0) # CRITICAL FIX: Sanitize NaNs (0 = Mean)\n",
    "        \n",
    "        split = 'train' if year in TRAIN_YEARS else 'val' if year in range(2022, 2024) else 'test'\n",
    "        \n",
    "        # Sequence Creation\n",
    "        for i in range(CFG.T_IN, len(data) - CFG.T_OUT + 1, CFG.STRIDE):\n",
    "            buffers[split][0].append(data[i-CFG.T_IN:i])\n",
    "            buffers[split][1].append(data[i:i+CFG.T_OUT])\n",
    "            flush(split)\n",
    "        ds.close()\n",
    "    \n",
    "    # Final Flush\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        if buffers[split][0]:\n",
    "            X = np.stack(buffers[split][0])\n",
    "            Y = np.stack(buffers[split][1])\n",
    "            np.save(CFG.BATCHED_DIR / split / f'X_batch_{counts[split]:04d}.npy', X)\n",
    "            np.save(CFG.BATCHED_DIR / split / f'Y_batch_{counts[split]:04d}.npy', Y)\n",
    "    print('\u2705 Preprocessing Complete.')\n",
    "\n",
    "# CRITICAL FIX: ALWAYS LOAD STATS\n",
    "print(\"Loading normalization statistics...\")\n",
    "if stats_path.exists():\n",
    "    stats = np.load(stats_path)\n",
    "    mean = stats['mean']\n",
    "    std = stats['std']\n",
    "    print(f\"Loaded Stats \u2014 Mean: {mean}, Std: {std}\")\n",
    "else:\n",
    "    # CRITICAL FIX: Do not allow training without valid stats\n",
    "    raise FileNotFoundError(f\"\u274c stats.npz not found at {stats_path}. Run preprocessing first!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 5. DATALOADER (FIXED DATA LEAK)\n",
    "# ============================================================================\n",
    "def get_batch_files(split):\n",
    "    split_dir = CFG.BATCHED_DIR / split\n",
    "    x_files = sorted(list(split_dir.glob('X_batch_*.npy')))\n",
    "    y_files = sorted(list(split_dir.glob('Y_batch_*.npy')))\n",
    "    return x_files, y_files\n",
    "\n",
    "class BatchedWeatherDataset(IterableDataset):\n",
    "    def __init__(self, split, shuffle=False):\n",
    "        self.split = split\n",
    "        self.shuffle = shuffle\n",
    "        self.x_files, self.y_files = get_batch_files(split)\n",
    "        \n",
    "    def __iter__(self):\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "        x_files = list(self.x_files)\n",
    "        y_files = list(self.y_files)\n",
    "        \n",
    "        if worker_info is not None:\n",
    "            per_worker = int(np.ceil(len(x_files) / float(worker_info.num_workers)))\n",
    "            iter_start = worker_info.id * per_worker\n",
    "            iter_end = min(iter_start + per_worker, len(x_files))\n",
    "            x_files = x_files[iter_start:iter_end]\n",
    "            y_files = y_files[iter_start:iter_end]\n",
    "            \n",
    "        file_indices = list(range(len(x_files)))\n",
    "        if self.shuffle: np.random.shuffle(file_indices)\n",
    "        \n",
    "        for idx in file_indices:\n",
    "            try:\n",
    "                X = np.load(x_files[idx], mmap_mode='r')\n",
    "                Y = np.load(y_files[idx], mmap_mode='r')\n",
    "                n = len(X)\n",
    "                indices = np.random.permutation(n) if self.shuffle else np.arange(n)\n",
    "                \n",
    "                for start in range(0, n, CFG.BATCH_SIZE):\n",
    "                    batch_idx = indices[start:start+CFG.BATCH_SIZE]\n",
    "                    \n",
    "                    # CRITICAL FIX: DO NOT DROP PARTIAL BATCHES\n",
    "                    # if len(batch_idx) < CFG.BATCH_SIZE and self.split == 'train': continue \n",
    "                    # Removed the above line to prevent data loss\n",
    "                    \n",
    "                    x_np = X[batch_idx]\n",
    "                    y_np = Y[batch_idx]\n",
    "                    \n",
    "                    if not x_np.flags['C_CONTIGUOUS']: x_np = np.ascontiguousarray(x_np)\n",
    "                    if not y_np.flags['C_CONTIGUOUS']: y_np = np.ascontiguousarray(y_np)\n",
    "                    \n",
    "                    # (B, T, H, W, C) -> (B, T, C, H, W)\n",
    "                    x = torch.from_numpy(x_np).float().permute(0, 1, 4, 2, 3)\n",
    "                    y = torch.from_numpy(y_np).float().permute(0, 1, 4, 2, 3)\n",
    "                    yield x, y\n",
    "            except Exception as e:\n",
    "                 print(f'Error loading file {x_files[idx]}: {e}')\n",
    "\n",
    "def get_loader(split, shuffle=False):\n",
    "    ds = BatchedWeatherDataset(split, shuffle=shuffle)\n",
    "    return DataLoader(ds, batch_size=None, num_workers=CFG.NUM_WORKERS, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 6. MODEL ARCHITECTURE\n",
    "# ============================================================================\n",
    "class ConvLSTMCell(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.conv = nn.Conv2d(input_dim + hidden_dim, 4 * hidden_dim, kernel_size, padding=kernel_size//2)\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        h, c = hidden\n",
    "        gates = self.conv(torch.cat([x, h], dim=1))\n",
    "        i, f, o, g = torch.chunk(gates, 4, dim=1)\n",
    "        c_next = torch.sigmoid(f) * c + torch.sigmoid(i) * torch.tanh(g)\n",
    "        h_next = torch.sigmoid(o) * torch.tanh(c_next)\n",
    "        return h_next, c_next\n",
    "    \n",
    "    def init_hidden(self, B, H, W, device):\n",
    "        return (torch.zeros(B, self.hidden_dim, H, W, device=device),\n",
    "                torch.zeros(B, self.hidden_dim, H, W, device=device))\n",
    "\n",
    "class WeatherNowcaster(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_dim, out_channels, n_layers=4):\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.encoder = nn.ModuleList([\n",
    "            ConvLSTMCell(in_channels if i==0 else hidden_dim, hidden_dim, 3)\n",
    "            for i in range(n_layers)\n",
    "        ])\n",
    "        self.decoder = nn.ModuleList([\n",
    "            ConvLSTMCell(out_channels if i==0 else hidden_dim, hidden_dim, 3)\n",
    "            for i in range(n_layers)\n",
    "        ])\n",
    "        self.out_conv = nn.Conv2d(hidden_dim, out_channels, 3, padding=1)\n",
    "        \n",
    "    def forward(self, x, future_steps):\n",
    "        B, T, C, H, W = x.shape\n",
    "        hidden = [cell.init_hidden(B, H, W, x.device) for cell in self.encoder]\n",
    "        \n",
    "        # Encode\n",
    "        for t in range(T):\n",
    "            inp = x[:, t]\n",
    "            for i, cell in enumerate(self.encoder):\n",
    "                h, c = cell(inp, hidden[i])\n",
    "                hidden[i] = (h, c)\n",
    "                inp = h\n",
    "        \n",
    "        # Decode\n",
    "        dec_hidden = [(h.clone(), c.clone()) for h, c in hidden]\n",
    "        outputs = []\n",
    "        dec_inp = self.out_conv(dec_hidden[-1][0]) # Initial projection\n",
    "        \n",
    "        for _ in range(future_steps):\n",
    "            inp = dec_inp\n",
    "            for i, cell in enumerate(self.decoder):\n",
    "                h, c = cell(inp, dec_hidden[i])\n",
    "                dec_hidden[i] = (h, c)\n",
    "                inp = h\n",
    "            \n",
    "            dec_out = self.out_conv(inp)\n",
    "            outputs.append(dec_out)\n",
    "            dec_inp = dec_out # Autoregressive\n",
    "            \n",
    "        return torch.stack(outputs, dim=1)\n",
    "\n",
    "# FIX: Move this function OUTSIDE the class\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        # Xavier/Glorot initialization is generally safer for Sigmoid/Tanh gates\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None: nn.init.zeros_(m.bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 8. LOSS & METRICS (PHYSICALLY GROUNDED & STABILIZED)\n",
    "# ============================================================================\n",
    "class WeightedMSE(nn.Module):\n",
    "    def __init__(self, mean, std, mm_threshold=1.0, nonzero_weight=3.0):\n",
    "        super().__init__()\n",
    "        self.nonzero_weight = nonzero_weight\n",
    "        \n",
    "        # CRITICAL FIX: Convert physical threshold (mm) to Z-score space\n",
    "        # Formula: z = (log1p(x) - mean) / std\n",
    "        # We assume mean/std are for the precip channel (index 0)\n",
    "        log_thresh = np.log1p(mm_threshold)\n",
    "        # Ensure we don't divide by zero if stats are weird\n",
    "        safe_std = std[0] if std[0] > 1e-6 else 1.0\n",
    "        self.z_threshold = (log_thresh - mean[0]) / safe_std\n",
    "        print(f\"WeightedMSE: Threshold {mm_threshold}mm -> Z-score {self.z_threshold:.4f}\")\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        # FIX 1: Force float32 for stability\n",
    "        pred = pred.float()\n",
    "        target = target.float()\n",
    "        \n",
    "        # 1. Handle Precipitation (Channel 0) with Weights\n",
    "        target_tp = target[:, :, 0]\n",
    "        pred_tp = pred[:, :, 0]\n",
    "        \n",
    "        # FIX 2: Apply weight ONLY to precip and based on Z-score threshold\n",
    "        # We use signed comparison (target_tp > z_threshold) because precip is one-sided\n",
    "        weight = torch.where(target_tp > self.z_threshold, \n",
    "                             self.nonzero_weight, \n",
    "                             1.0) # Base weight is 1.0\n",
    "        \n",
    "        mse_tp = (weight * (pred_tp - target_tp)**2).mean()\n",
    "        \n",
    "        # 2. Handle Temperature (Channel 1) normally (No weights)\n",
    "        mse_t2m = (pred[:, :, 1] - target[:, :, 1])**2 .mean()\n",
    "        \n",
    "        return mse_tp + mse_t2m\n",
    "\n",
    "class SSIMLoss(nn.Module):\n",
    "    def __init__(self, window_size=7, channels=2):\n",
    "        super().__init__()\n",
    "        self.window_size = window_size\n",
    "        self.channels = channels\n",
    "        sigma = 1.5\n",
    "        coords = torch.arange(window_size).float() - window_size // 2\n",
    "        g = torch.exp(-(coords**2) / (2*sigma**2))\n",
    "        g = g / g.sum()\n",
    "        self.window = (g.unsqueeze(1) @ g.unsqueeze(0)).unsqueeze(0).unsqueeze(0)\n",
    "        self.window = self.window.expand(channels, 1, -1, -1).contiguous()\n",
    "        \n",
    "    def forward(self, pred, target):\n",
    "        # FIX: CRITICAL - Force float32 to prevent -inf in float16 AMP\n",
    "        pred = pred.float()\n",
    "        target = target.float()\n",
    "        \n",
    "        if pred.dim() == 5:\n",
    "            B, T, C, H, W = pred.shape\n",
    "            pred = pred.reshape(B * T, C, H, W)\n",
    "            target = target.reshape(B * T, C, H, W)\n",
    "            \n",
    "        window = self.window.to(pred.device)\n",
    "        \n",
    "        mu1 = F.conv2d(pred, window, padding=self.window_size//2, groups=self.channels)\n",
    "        mu2 = F.conv2d(target, window, padding=self.window_size//2, groups=self.channels)\n",
    "        \n",
    "        mu1_sq = mu1.pow(2)\n",
    "        mu2_sq = mu2.pow(2)\n",
    "        mu1_mu2 = mu1 * mu2\n",
    "        \n",
    "        sigma1_sq = F.conv2d(pred*pred, window, padding=self.window_size//2, groups=self.channels) - mu1_sq\n",
    "        sigma2_sq = F.conv2d(target*target, window, padding=self.window_size//2, groups=self.channels) - mu2_sq\n",
    "        \n",
    "        # Clamp variance to 0 (mathematically required)\n",
    "        sigma1_sq = torch.clamp(sigma1_sq, min=0.0)\n",
    "        sigma2_sq = torch.clamp(sigma2_sq, min=0.0)\n",
    "        sigma12 = F.conv2d(pred*target, window, padding=self.window_size//2, groups=self.channels) - mu1_mu2\n",
    "        \n",
    "        C1 = (0.01 * 6)**2\n",
    "        C2 = (0.03 * 6)**2\n",
    "        \n",
    "        numerator = (2 * mu1_mu2 + C1) * (2 * sigma12 + C2)\n",
    "        denominator = (mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2)\n",
    "        \n",
    "        ssim_map = numerator / (denominator + 1e-7)\n",
    "        \n",
    "        # Clamp final result to [0, 2] range to be safe\n",
    "        return torch.clamp(1 - ssim_map.mean(), min=0.0, max=2.0)\n",
    "\n",
    "# Initialize Loss with Stats (Passed from preprocessing)\n",
    "# We assume 'mean' and 'std' variables are available from Section 4\n",
    "wmse = WeightedMSE(mean=mean, std=std, mm_threshold=1.0, nonzero_weight=CFG.PRECIP_WEIGHT)\n",
    "ssim = SSIMLoss(channels=CFG.OUT_CHANNELS)\n",
    "mae = nn.L1Loss()\n",
    "\n",
    "def criterion(pred, target):\n",
    "    return 0.5 * wmse(pred, target) + 0.3 * ssim(pred, target) + 0.2 * mae(pred, target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 9. TRAINING LOOP (ROBUST)\n",
    "# ============================================================================\n",
    "model = WeatherNowcaster(CFG.IN_CHANNELS, CFG.HIDDEN_DIM, CFG.OUT_CHANNELS, CFG.N_LAYERS).to(device)\n",
    "model.apply(init_weights)\n",
    "print(f'\ud83e\udde0 Model Parameters: {sum(p.numel() for p in model.parameters())/1e6:.2f} M')\n",
    "\n",
    "from torch.amp import autocast, GradScaler\n",
    "scaler = GradScaler('cuda')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=CFG.LEARNING_RATE, weight_decay=CFG.WEIGHT_DECAY)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=CFG.NUM_EPOCHS, eta_min=1e-6)\n",
    "\n",
    "# Resume Logic\n",
    "start_epoch = 0\n",
    "train_losses, val_losses = [], []\n",
    "resume_path = SAVE_DIR / 'last.pth'\n",
    "if resume_path.exists():\n",
    "    print(f\"\ud83d\udcc2 Resuming from {resume_path}\")\n",
    "    try:\n",
    "        ckpt = torch.load(resume_path, map_location=device)\n",
    "        model.load_state_dict(ckpt['model'])\n",
    "        optimizer.load_state_dict(ckpt['optimizer'])\n",
    "        start_epoch = ckpt['epoch'] + 1\n",
    "        train_losses = ckpt.get('train_losses', [])\n",
    "        val_losses = ckpt.get('val_losses', [])\n",
    "        print(f\"\u25b6\ufe0f Resumed at Epoch {start_epoch}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\u26a0\ufe0f Checkpoint load failed: {e}. Starting fresh.\")\n",
    "\n",
    "train_loader = get_loader('train', shuffle=True)\n",
    "val_loader = get_loader('val', shuffle=False)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "print('\ud83d\udd25 Starting Training...')\n",
    "for epoch in range(start_epoch, CFG.NUM_EPOCHS):\n",
    "    model.train()\n",
    "    ep_loss = 0\n",
    "    count = 0\n",
    "    nan_counter = 0  # Track bad batches\n",
    "    \n",
    "    n_train_files = len(list((CFG.BATCHED_DIR / 'train').glob('X_batch_*.npy')))\n",
    "    total_batches = (n_train_files * 500) // CFG.BATCH_SIZE\n",
    "    \n",
    "    pbar = tqdm(train_loader, total=total_batches, desc=f'Epoch {epoch+1}/{CFG.NUM_EPOCHS} [Train]')\n",
    "    \n",
    "    for x, y in pbar:\n",
    "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "        # ENABLED AMP FOR SPEED\n",
    "        with autocast('cuda'):\n",
    "            pred = model(x, CFG.T_OUT)\n",
    "            loss = criterion(pred, y)\n",
    "            \n",
    "        # Robust Safety Check (Catches both NaN and Inf)\n",
    "        if not torch.isfinite(loss):\n",
    "            nan_counter += 1\n",
    "            if nan_counter <= 3: # Only print first 3 warnings\n",
    "                print(f\"\u26a0\ufe0f Non-finite loss detected! Skipping batch.\")\n",
    "            continue\n",
    "            \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.MAX_GRAD_NORM)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        ep_loss += loss.item()\n",
    "        count += 1\n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    if nan_counter > 0:\n",
    "        print(f\"\u26a0\ufe0f Total skipped batches this epoch: {nan_counter}\")\n",
    "\n",
    "    train_loss = ep_loss / count if count > 0 else 0\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss_sum = 0\n",
    "    val_count = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in tqdm(val_loader, desc=f'Epoch {epoch+1} [Val]', leave=False):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            with autocast('cuda'):\n",
    "                pred = model(x, CFG.T_OUT)\n",
    "                val_loss_sum += criterion(pred, y).item()\n",
    "            val_count += 1\n",
    "    \n",
    "    val_loss = val_loss_sum / val_count if val_count > 0 else 0\n",
    "    val_losses.append(val_loss)\n",
    "    scheduler.step()\n",
    "    \n",
    "    print(f'Epoch {epoch+1} | Train: {train_loss:.4f} | Val: {val_loss:.4f}')\n",
    "    \n",
    "    # SAVE LOCALLY (Fast)\n",
    "    state = {\n",
    "        'epoch': epoch,\n",
    "        'model': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses\n",
    "    }\n",
    "    torch.save(state, SAVE_DIR / 'last.pth')\n",
    "    \n",
    "    # BACKUP TO DRIVE\n",
    "    if DRIVE_AVAILABLE:\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(state, CFG.DRIVE_CKPT_DIR / 'best_model.pth')\n",
    "            torch.save(state, SAVE_DIR / 'best_model.pth')\n",
    "            print('  \u2605 New Best Model Synced to Drive!')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        if (epoch + 1) % 5 == 0:\n",
    "             shutil.copy(SAVE_DIR / 'last.pth', CFG.DRIVE_CKPT_DIR / 'last_backup.pth')\n",
    "        \n",
    "        if patience_counter >= CFG.PATIENCE:\n",
    "            print(f'\\n\u23f9\ufe0f Early stopping triggered.')\n",
    "            break\n",
    "\n",
    "print('\u2705 Training Complete.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TRAINING PLOTS\n",
    "# ============================================================================\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(train_losses, label='Train Loss')\n",
    "axes[0].plot(val_losses, label='Val Loss')\n",
    "axes[0].set_title('Loss Curves')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EVALUATION & VISUALIZATION (PAPER-READY)\n",
    "# ============================================================================\n",
    "print('Loading Best Model for Evaluation...')\n",
    "# Logic: Try Drive Best -> Local Best -> Local Last\n",
    "best_path = None\n",
    "if DRIVE_AVAILABLE and (CFG.DRIVE_CKPT_DIR / 'best_model.pth').exists():\n",
    "    best_path = CFG.DRIVE_CKPT_DIR / 'best_model.pth'\n",
    "elif (SAVE_DIR / 'best_model.pth').exists():\n",
    "    best_path = SAVE_DIR / 'best_model.pth'\n",
    "else:\n",
    "    best_path = SAVE_DIR / 'last.pth'\n",
    "\n",
    "if best_path and best_path.exists():\n",
    "    ckpt = torch.load(best_path, map_location=device)\n",
    "    model.load_state_dict(ckpt['model'])\n",
    "    print(f'Loaded model from {best_path} (Epoch {ckpt[\"epoch\"]+1})')\n",
    "else:\n",
    "    print('\u26a0\ufe0f No model found for evaluation.')\n",
    "\n",
    "model.eval()\n",
    "test_loader = get_loader('test', shuffle=False)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 1. QUANTITATIVE EVALUATION (METRICS vs BASELINE)\n",
    "# ----------------------------------------------------------------------------\n",
    "def compute_metrics(pred, target, threshold=0.5):\n",
    "    pred_b = (pred > threshold).float()\n",
    "    target_b = (target > threshold).float()\n",
    "    \n",
    "    hits = (pred_b * target_b).sum()\n",
    "    misses = ((1-pred_b)*target_b).sum()\n",
    "    false_alarms = (pred_b*(1-target_b)).sum()\n",
    "    \n",
    "    csi = hits / (hits + misses + false_alarms + 1e-8)\n",
    "    pod = hits / (hits + misses + 1e-8)\n",
    "    far = false_alarms / (hits + false_alarms + 1e-8)\n",
    "    bias = (hits + false_alarms) / (hits + misses + 1e-8)\n",
    "    \n",
    "    return csi.item(), pod.item(), far.item(), bias.item()\n",
    "\n",
    "print('Computing Metrics on Test Set...')\n",
    "# Define thresholds in MM (Physical Space)\n",
    "THRESHOLDS_MM = [0.5, 2.0, 5.0] \n",
    "results = {thresh: {'csi': [], 'pod': [], 'far': [], 'bias': []} for thresh in THRESHOLDS_MM}\n",
    "baseline_results = {thresh: {'csi': [], 'pod': [], 'far': [], 'bias': []} for thresh in THRESHOLDS_MM}\n",
    "mse_scores = []\n",
    "baseline_mse_scores = []\n",
    "\n",
    "# Stats for denormalization\n",
    "tp_mean = mean[0]\n",
    "tp_std = std[0]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in tqdm(test_loader, desc='Evaluating'):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        # Model Prediction\n",
    "        with torch.amp.autocast('cuda'):\n",
    "             pred = model(x, CFG.T_OUT)\n",
    "        \n",
    "        pred = pred.float()\n",
    "        \n",
    "        # ---------------------------------------------------------\n",
    "        # CRITICAL FIX: DENORMALIZE TO PHYSICAL SPACE (MM)\n",
    "        # ---------------------------------------------------------\n",
    "        # Z-score inverse: val = z * std + mean\n",
    "        # Log1p inverse: mm = exp(val) - 1\n",
    "        \n",
    "        # 1. Denormalize Prediction\n",
    "        pred_z = pred[:, :, 0] # Channel 0 is TP\n",
    "        pred_denorm = pred_z * tp_std + tp_mean\n",
    "        pred_mm = torch.expm1(pred_denorm) # Inverse of log1p\n",
    "        pred_mm = torch.clamp(pred_mm, min=0.0) # Rain cannot be negative\n",
    "        \n",
    "        # 2. Denormalize Target\n",
    "        target_z = y[:, :, 0]\n",
    "        target_denorm = target_z * tp_std + tp_mean\n",
    "        target_mm = torch.expm1(target_denorm)\n",
    "        \n",
    "        # 3. Denormalize Baseline (Persistence)\n",
    "        # Baseline is just the last frame of input sequence x\n",
    "        # x is still normalized. We need to denormalize it.\n",
    "        last_frame_z = x[:, -1, 0] # Shape (B, H, W)\n",
    "        last_frame_denorm = last_frame_z * tp_std + tp_mean\n",
    "        persistence_mm = torch.expm1(last_frame_denorm)\n",
    "        persistence_mm = torch.clamp(persistence_mm, min=0.0)\n",
    "        # Repeat for T_OUT steps\n",
    "        persistence_mm = persistence_mm.unsqueeze(1).repeat(1, CFG.T_OUT, 1, 1)\n",
    "        \n",
    "        mse_scores.append(F.mse_loss(pred_mm, target_mm).item())\n",
    "        baseline_mse_scores.append(F.mse_loss(persistence_mm, target_mm).item())\n",
    "        \n",
    "        # Categorical Metrics (Now using MM thresholds)\n",
    "        for thresh in THRESHOLDS_MM:\n",
    "            # Model\n",
    "            c, p, f, b = compute_metrics(pred_mm, target_mm, threshold=thresh)\n",
    "            results[thresh]['csi'].append(c)\n",
    "            results[thresh]['pod'].append(p)\n",
    "            results[thresh]['far'].append(f)\n",
    "            results[thresh]['bias'].append(b)\n",
    "            \n",
    "            # Baseline\n",
    "            bc, bp, bf, bb = compute_metrics(persistence_mm, target_mm, threshold=thresh)\n",
    "            baseline_results[thresh]['csi'].append(bc)\n",
    "            baseline_results[thresh]['pod'].append(bp)\n",
    "            baseline_results[thresh]['far'].append(bf)\n",
    "            baseline_results[thresh]['bias'].append(bb)\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "print('TEST SET RESULTS (Physical Space - mm/hr)')\n",
    "print('='*60)\n",
    "print(f'MSE (mm/hr^2) | Model: {np.mean(mse_scores):.4f} | Persistence: {np.mean(baseline_mse_scores):.4f}')\n",
    "print('-'*60)\n",
    "print(f'{ \"Threshold\":<10} | { \"Metric\":<5} | { \"Model\":<10} | { \"Persistence\":<10} | { \"Improvement\":<10}')\n",
    "print('-'*60)\n",
    "for t in THRESHOLDS_MM:\n",
    "    for m in ['csi', 'pod', 'far']:\n",
    "        score = np.mean(results[t][m])\n",
    "        base = np.mean(baseline_results[t][m])\n",
    "        imp = (score - base) if m != 'far' else (base - score)\n",
    "        print(f'{t:<10} | {m.upper():<5} | {score:.4f}     | {base:.4f}           | {imp:+.4f}')\n",
    "    print('-'*60)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 2. QUALITATIVE VISUALIZATION (ERROR MAPS)\n",
    "# ----------------------------------------------------------------------------\n",
    "def visualize_prediction(x, y, pred, idx=0):\n",
    "    \"\"\"Show Input, Target, Pred, Error\"\"\"\n",
    "    fig, axes = plt.subplots(4, 7, figsize=(20, 10))\n",
    "    \n",
    "    # Input (Stats Denorm)\n",
    "    for i, t in enumerate([21, 22, 23]):\n",
    "        ax = axes[0, i]\n",
    "        val = x[idx, t, 0].cpu().float() * tp_std + tp_mean\n",
    "        val = torch.expm1(val).clamp(0)\n",
    "        ax.imshow(val, cmap='Blues', origin='lower')\n",
    "        ax.set_title(f'Input t={t-23}')\n",
    "        ax.axis('off')\n",
    "    axes[0,3].text(0.5, 0.5, 'INPUT SEQUENCE', ha='center', fontsize=12); axes[0,3].axis('off')\n",
    "    \n",
    "    # Target\n",
    "    for i in range(6):\n",
    "        ax = axes[1, i]\n",
    "        val = y[idx, i, 0].cpu().float() * tp_std + tp_mean\n",
    "        val = torch.expm1(val).clamp(0)\n",
    "        ax.imshow(val, cmap='Blues', origin='lower')\n",
    "        ax.set_title(f'Target t+{i+1}')\n",
    "        ax.axis('off')\n",
    "    axes[1,6].text(0, 0.5, 'Ground Truth', rotation=90, va='center', fontsize=12); axes[1,6].axis('off')\n",
    "        \n",
    "    # Pred\n",
    "    for i in range(6):\n",
    "        ax = axes[2, i]\n",
    "        val = pred[idx, i, 0].detach().cpu().float() * tp_std + tp_mean\n",
    "        val = torch.expm1(val).clamp(0)\n",
    "        ax.imshow(val, cmap='Blues', origin='lower')\n",
    "        ax.set_title(f'Pred t+{i+1}')\n",
    "        ax.axis('off')\n",
    "    axes[2,6].text(0, 0.5, 'Prediction', rotation=90, va='center', fontsize=12); axes[2,6].axis('off')\n",
    "\n",
    "    # Error\n",
    "    for i in range(6):\n",
    "        ax = axes[3, i]\n",
    "        t_val = torch.expm1(y[idx, i, 0].cpu().float()*tp_std+tp_mean).clamp(0)\n",
    "        p_val = torch.expm1(pred[idx, i, 0].detach().cpu().float()*tp_std+tp_mean).clamp(0)\n",
    "        diff = torch.abs(t_val - p_val)\n",
    "        ax.imshow(diff, cmap='hot', origin='lower', vmin=0, vmax=5)\n",
    "        ax.set_title(f'Error t+{i+1}')\n",
    "        ax.axis('off')\n",
    "    axes[3,6].text(0, 0.5, '|Target - Pred|', rotation=90, va='center', fontsize=12); axes[3,6].axis('off')\n",
    "    \n",
    "    plt.suptitle('Model Evaluation (Physical Space - mm)', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print('Running Inference for Visualization...')\n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        with torch.amp.autocast('cuda'): pred = model(x, CFG.T_OUT)\n",
    "        visualize_prediction(x, y, pred.float(), idx=0)\n",
    "        break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}