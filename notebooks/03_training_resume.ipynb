{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 03_training.ipynb - GITHUB DATA SOURCE VERSION\n",
                "\n",
                "**This notebook:**\n",
                "1. Downloads raw NetCDF data directly from GitHub (both `Dataset/` and `2025 data/` folders)\n",
                "2. Preprocesses and batches the data **in streaming mode** (low memory usage)\n",
                "3. Trains the WeatherNowcaster model with Mixed Precision (AMP)\n",
                "\n",
                "**Prerequisites:**\n",
                "- Enable GPU: Runtime \u2192 Change runtime type \u2192 T4 GPU\n",
                "\n",
                "**Expected: ~2-5 min per epoch on T4**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 1: Setup & GPU Verification\n",
                "!pip install -q netCDF4\n",
                "\n",
                "import os, gc, glob, time, requests, zipfile, io\n",
                "import numpy as np\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "import matplotlib.pyplot as plt\n",
                "from tqdm.auto import tqdm\n",
                "from datetime import datetime\n",
                "\n",
                "# CRITICAL: Verify GPU\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f'Device: {device}')\n",
                "if device.type != 'cuda':\n",
                "    print('\u26a0\ufe0f WARNING: Running on CPU! Enable GPU: Runtime \u2192 Change runtime type \u2192 GPU')\n",
                "else:\n",
                "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
                "    print(f'VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')\n",
                "    torch.backends.cudnn.benchmark = True\n",
                "    print('\u2713 cudnn.benchmark enabled')\n",
                "\n",
                "# Mixed Precision\n",
                "from torch.cuda.amp import autocast, GradScaler\n",
                "scaler = GradScaler()\n",
                "print('\u2713 Mixed Precision (AMP) enabled')\n",
                "\n",
                "# Paths\n",
                "WORK_DIR = '/content/weather_nowcasting'\n",
                "DATA_DIR = f'{WORK_DIR}/Dataset'\n",
                "DATA_2025_DIR = f'{WORK_DIR}/2025_data'\n",
                "BATCHED_DIR = f'{WORK_DIR}/data/batched'\n",
                "CHECKPOINT_DIR = f'{WORK_DIR}/checkpoints'\n",
                "FIGURES_DIR = f'{WORK_DIR}/figures'\n",
                "\n",
                "os.makedirs(DATA_DIR, exist_ok=True)\n",
                "os.makedirs(DATA_2025_DIR, exist_ok=True)\n",
                "os.makedirs(BATCHED_DIR, exist_ok=True)\n",
                "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
                "os.makedirs(FIGURES_DIR, exist_ok=True)\n",
                "\n",
                "print(f'\\nWork directory: {WORK_DIR}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 2: Download Dataset from GitHub\n",
                "GITHUB_REPO = 'ui07xWizardOp/paper-weather-nowcasting'\n",
                "BRANCH = 'main'\n",
                "\n",
                "def download_github_repo():\n",
                "    \"\"\"Download the entire repository as a ZIP and extract Dataset + 2025 data folders.\"\"\"\n",
                "    zip_url = f'https://github.com/{GITHUB_REPO}/archive/refs/heads/{BRANCH}.zip'\n",
                "    print(f'Downloading from: {zip_url}')\n",
                "    \n",
                "    response = requests.get(zip_url, stream=True)\n",
                "    if response.status_code != 200:\n",
                "        raise Exception(f'Failed to download: HTTP {response.status_code}')\n",
                "    \n",
                "    total_size = int(response.headers.get('content-length', 0))\n",
                "    print(f'Download size: {total_size / 1e6:.1f} MB')\n",
                "    \n",
                "    chunks = []\n",
                "    for chunk in tqdm(response.iter_content(chunk_size=8192), \n",
                "                      total=total_size//8192, desc='Downloading'):\n",
                "        chunks.append(chunk)\n",
                "    \n",
                "    zip_data = b''.join(chunks)\n",
                "    print(f'Downloaded {len(zip_data) / 1e6:.1f} MB')\n",
                "    \n",
                "    with zipfile.ZipFile(io.BytesIO(zip_data), 'r') as zf:\n",
                "        print('\\nExtracting Dataset folder (2015-2024 data)...')\n",
                "        dataset_files = [f for f in zf.namelist() if '/Dataset/' in f and f.endswith('.nc')]\n",
                "        print(f'Found {len(dataset_files)} NetCDF files in Dataset/')\n",
                "        \n",
                "        for file in tqdm(dataset_files, desc='Extracting Dataset'):\n",
                "            filename = os.path.basename(file)\n",
                "            if filename:\n",
                "                with zf.open(file) as src:\n",
                "                    with open(f'{DATA_DIR}/{filename}', 'wb') as dst:\n",
                "                        dst.write(src.read())\n",
                "        \n",
                "        print('\\nExtracting 2025 data folder...')\n",
                "        data_2025_files = [f for f in zf.namelist() if '/2025 data/' in f and f.endswith('.nc')]\n",
                "        print(f'Found {len(data_2025_files)} NetCDF files in 2025 data/')\n",
                "        \n",
                "        for file in tqdm(data_2025_files, desc='Extracting 2025 data'):\n",
                "            filename = os.path.basename(file)\n",
                "            if filename:\n",
                "                with zf.open(file) as src:\n",
                "                    with open(f'{DATA_2025_DIR}/{filename}', 'wb') as dst:\n",
                "                        dst.write(src.read())\n",
                "    \n",
                "    del zip_data, chunks\n",
                "    gc.collect()\n",
                "    \n",
                "    main_files = glob.glob(f'{DATA_DIR}/*.nc')\n",
                "    files_2025 = glob.glob(f'{DATA_2025_DIR}/*.nc')\n",
                "    print(f'\\n\u2713 Extracted {len(main_files)} files to Dataset/')\n",
                "    print(f'\u2713 Extracted {len(files_2025)} files to 2025_data/')\n",
                "    return main_files, files_2025\n",
                "\n",
                "existing_main = glob.glob(f'{DATA_DIR}/*.nc')\n",
                "existing_2025 = glob.glob(f'{DATA_2025_DIR}/*.nc')\n",
                "\n",
                "if len(existing_main) >= 100 and len(existing_2025) >= 10:\n",
                "    print(f'\u2713 Dataset already exists:')\n",
                "    print(f'  - Main (2015-2024): {len(existing_main)} files')\n",
                "    print(f'  - 2025 data: {len(existing_2025)} files')\n",
                "else:\n",
                "    existing_main, existing_2025 = download_github_repo()\n",
                "\n",
                "print(f'\\nTotal NetCDF files: {len(existing_main) + len(existing_2025)}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 3: Memory-Efficient Preprocessing Configuration\n",
                "import xarray as xr\n",
                "import pandas as pd\n",
                "\n",
                "# Configuration\n",
                "T_IN = 24\n",
                "T_OUT = 6\n",
                "STRIDE = 6\n",
                "SAMPLES_PER_BATCH = 500\n",
                "VARIABLES = ['tp', 't2m']\n",
                "\n",
                "TRAIN_YEARS = list(range(2015, 2022))\n",
                "VAL_YEARS = list(range(2022, 2024))\n",
                "TEST_YEARS = list(range(2024, 2027))\n",
                "\n",
                "def load_single_file(filepath):\n",
                "    \"\"\"Load a NetCDF file, handling different coordinate naming conventions.\"\"\"\n",
                "    # Try different engines\n",
                "    for engine in ['netcdf4', 'h5netcdf', 'scipy', None]:\n",
                "        try:\n",
                "            if engine:\n",
                "                ds = xr.open_dataset(filepath, engine=engine)\n",
                "            else:\n",
                "                ds = xr.open_dataset(filepath)\n",
                "            break\n",
                "        except Exception:\n",
                "            continue\n",
                "    else:\n",
                "        raise ValueError(f'Could not open {filepath} with any engine')\n",
                "    \n",
                "    # Handle 'valid_time' vs 'time' naming\n",
                "    if 'valid_time' in ds.coords and 'time' not in ds.coords:\n",
                "        ds = ds.rename({'valid_time': 'time'})\n",
                "    \n",
                "    # Handle 'expver' dimension (experimental version)\n",
                "    if 'expver' in ds.dims:\n",
                "        ds = ds.isel(expver=0, drop=True)\n",
                "    elif 'expver' in ds.coords:\n",
                "        ds = ds.drop_vars('expver', errors='ignore')\n",
                "    \n",
                "    # Handle 'number' coordinate\n",
                "    if 'number' in ds.coords:\n",
                "        ds = ds.drop_vars('number', errors='ignore')\n",
                "    \n",
                "    return ds\n",
                "\n",
                "# Check if batched data already exists\n",
                "batched_train = glob.glob(f'{BATCHED_DIR}/train/X_batch_*.npy')\n",
                "if len(batched_train) > 0:\n",
                "    print(f'\u2713 Batched data already exists: {len(batched_train)} train batches')\n",
                "    print('  Skipping preprocessing...')\n",
                "    PREPROCESSING_DONE = True\n",
                "else:\n",
                "    PREPROCESSING_DONE = False\n",
                "    print('Will run memory-efficient preprocessing...')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 4: MEMORY-EFFICIENT Preprocessing (Streaming per-file)\n",
                "if not PREPROCESSING_DONE:\n",
                "    all_nc_files = sorted(glob.glob(f'{DATA_DIR}/*.nc')) + sorted(glob.glob(f'{DATA_2025_DIR}/*.nc'))\n",
                "    print(f'Total files to process: {len(all_nc_files)}')\n",
                "    \n",
                "    # Helper: extract variables from dataset\n",
                "    def extract_vars(ds):\n",
                "        var_data = []\n",
                "        for var in VARIABLES:\n",
                "            if var in ds.data_vars:\n",
                "                var_data.append(ds[var].values)\n",
                "            elif var == 't2m' and '2m_temperature' in ds.data_vars:\n",
                "                var_data.append(ds['2m_temperature'].values)\n",
                "            elif var == 'tp' and 'total_precipitation' in ds.data_vars:\n",
                "                var_data.append(ds['total_precipitation'].values)\n",
                "        if len(var_data) == len(VARIABLES):\n",
                "            return np.stack(var_data, axis=-1).astype(np.float32)\n",
                "        return None\n",
                "    \n",
                "    # ========== PASS 1: Compute normalization stats from training data ==========\n",
                "    print('\\n[Pass 1] Computing normalization statistics...')\n",
                "    train_values = []\n",
                "    files_loaded = 0\n",
                "    \n",
                "    for f in tqdm(all_nc_files, desc='Scanning for stats'):\n",
                "        try:\n",
                "            ds = load_single_file(f)\n",
                "            year = pd.to_datetime(ds.time.values[0]).year\n",
                "            \n",
                "            if year in TRAIN_YEARS:\n",
                "                data = extract_vars(ds)\n",
                "                if data is not None:\n",
                "                    # Sample every 24th hour to reduce memory\n",
                "                    train_values.append(data[::24])\n",
                "                    files_loaded += 1\n",
                "            ds.close()\n",
                "        except Exception as e:\n",
                "            print(f'Warning: {os.path.basename(f)}: {e}')\n",
                "    \n",
                "    print(f'Loaded stats from {files_loaded} training files')\n",
                "    \n",
                "    if not train_values:\n",
                "        raise ValueError('No training data found! Check file formats.')\n",
                "    \n",
                "    train_sample = np.concatenate(train_values, axis=0)\n",
                "    mean = np.nanmean(train_sample, axis=(0, 1, 2))\n",
                "    std = np.nanstd(train_sample, axis=(0, 1, 2))\n",
                "    std[std < 1e-6] = 1.0\n",
                "    print(f'Mean: {mean}')\n",
                "    print(f'Std: {std}')\n",
                "    del train_values, train_sample\n",
                "    gc.collect()\n",
                "    \n",
                "    # ========== PASS 2: Process files and create sequences ==========\n",
                "    print('\\n[Pass 2] Processing files and creating sequences...')\n",
                "    \n",
                "    for split in ['train', 'val', 'test']:\n",
                "        os.makedirs(f'{BATCHED_DIR}/{split}', exist_ok=True)\n",
                "    \n",
                "    # Accumulators\n",
                "    X_buffers = {'train': [], 'val': [], 'test': []}\n",
                "    Y_buffers = {'train': [], 'val': [], 'test': []}\n",
                "    batch_counters = {'train': 0, 'val': 0, 'test': 0}\n",
                "    \n",
                "    def save_buffer(split):\n",
                "        \"\"\"Save accumulated buffer to disk.\"\"\"\n",
                "        if len(X_buffers[split]) >= SAMPLES_PER_BATCH:\n",
                "            X_batch = np.stack(X_buffers[split][:SAMPLES_PER_BATCH])\n",
                "            Y_batch = np.stack(Y_buffers[split][:SAMPLES_PER_BATCH])\n",
                "            np.save(f'{BATCHED_DIR}/{split}/X_batch_{batch_counters[split]:04d}.npy', X_batch)\n",
                "            np.save(f'{BATCHED_DIR}/{split}/Y_batch_{batch_counters[split]:04d}.npy', Y_batch)\n",
                "            batch_counters[split] += 1\n",
                "            X_buffers[split] = X_buffers[split][SAMPLES_PER_BATCH:]\n",
                "            Y_buffers[split] = Y_buffers[split][SAMPLES_PER_BATCH:]\n",
                "    \n",
                "    def create_sequences(data, t_in, t_out):\n",
                "        \"\"\"Create sequences from contiguous data.\"\"\"\n",
                "        X_list, Y_list = [], []\n",
                "        n = len(data)\n",
                "        if n < t_in + t_out:\n",
                "            return [], []\n",
                "        for i in range(t_in, n - t_out + 1):\n",
                "            X_list.append(data[i-t_in:i])\n",
                "            Y_list.append(data[i:i+t_out])\n",
                "        return X_list, Y_list\n",
                "    \n",
                "    # Process each file\n",
                "    for f in tqdm(all_nc_files, desc='Processing files'):\n",
                "        try:\n",
                "            ds = load_single_file(f)\n",
                "            data = extract_vars(ds)\n",
                "            \n",
                "            if data is None:\n",
                "                ds.close()\n",
                "                continue\n",
                "            \n",
                "            # Get year from first timestamp\n",
                "            year = pd.to_datetime(ds.time.values[0]).year\n",
                "            ds.close()\n",
                "            \n",
                "            # Normalize\n",
                "            data = np.nan_to_num(data, nan=0.0)\n",
                "            data = (data - mean) / std\n",
                "            \n",
                "            # Create sequences\n",
                "            X_seqs, Y_seqs = create_sequences(data, T_IN, T_OUT)\n",
                "            \n",
                "            # Determine split\n",
                "            if year in TRAIN_YEARS:\n",
                "                split = 'train'\n",
                "            elif year in VAL_YEARS:\n",
                "                split = 'val'\n",
                "            elif year in TEST_YEARS:\n",
                "                split = 'test'\n",
                "            else:\n",
                "                continue\n",
                "            \n",
                "            # Add to buffer\n",
                "            X_buffers[split].extend(X_seqs)\n",
                "            Y_buffers[split].extend(Y_seqs)\n",
                "            \n",
                "            # Save if buffer full\n",
                "            while len(X_buffers[split]) >= SAMPLES_PER_BATCH:\n",
                "                save_buffer(split)\n",
                "            \n",
                "            del data, X_seqs, Y_seqs\n",
                "        \n",
                "        except Exception as e:\n",
                "            print(f'Warning: {os.path.basename(f)}: {e}')\n",
                "    \n",
                "    # Final flush - save remaining sequences\n",
                "    for split in ['train', 'val', 'test']:\n",
                "        if X_buffers[split]:\n",
                "            X_batch = np.stack(X_buffers[split])\n",
                "            Y_batch = np.stack(Y_buffers[split])\n",
                "            np.save(f'{BATCHED_DIR}/{split}/X_batch_{batch_counters[split]:04d}.npy', X_batch)\n",
                "            np.save(f'{BATCHED_DIR}/{split}/Y_batch_{batch_counters[split]:04d}.npy', Y_batch)\n",
                "            batch_counters[split] += 1\n",
                "    \n",
                "    # Save stats\n",
                "    np.savez(f'{BATCHED_DIR}/stats.npz', mean=mean, std=std, variables=np.array(VARIABLES))\n",
                "    \n",
                "    print(f'\\n\u2713 Preprocessing complete!')\n",
                "    print(f'  Train: {batch_counters[\"train\"]} batches')\n",
                "    print(f'  Val: {batch_counters[\"val\"]} batches')\n",
                "    print(f'  Test: {batch_counters[\"test\"]} batches')\n",
                "    \n",
                "    gc.collect()\n",
                "else:\n",
                "    print('Using existing batched data.')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 5: Data Loading Utilities\n",
                "BATCH_SIZE = 32\n",
                "\n",
                "def get_batch_files(split):\n",
                "    split_dir = f'{BATCHED_DIR}/{split}'\n",
                "    x_files = sorted(glob.glob(f'{split_dir}/X_batch_*.npy'))\n",
                "    y_files = sorted(glob.glob(f'{split_dir}/Y_batch_*.npy'))\n",
                "    return x_files, y_files\n",
                "\n",
                "def batch_generator(split, batch_size=BATCH_SIZE, shuffle=False):\n",
                "    x_files, y_files = get_batch_files(split)\n",
                "    file_indices = list(range(len(x_files)))\n",
                "    if shuffle:\n",
                "        np.random.shuffle(file_indices)\n",
                "    \n",
                "    for idx in file_indices:\n",
                "        X = np.load(x_files[idx])\n",
                "        Y = np.load(y_files[idx])\n",
                "        n = len(X)\n",
                "        indices = np.random.permutation(n) if shuffle else np.arange(n)\n",
                "        \n",
                "        for start in range(0, n, batch_size):\n",
                "            batch_idx = indices[start:start+batch_size]\n",
                "            x = torch.from_numpy(X[batch_idx]).float().permute(0, 1, 4, 2, 3)\n",
                "            y = torch.from_numpy(Y[batch_idx]).float().permute(0, 1, 4, 2, 3)\n",
                "            yield x, y\n",
                "\n",
                "# Load stats\n",
                "stats = np.load(f'{BATCHED_DIR}/stats.npz', allow_pickle=True)\n",
                "mean, std = stats['mean'], stats['std']\n",
                "variables = list(stats['variables'])\n",
                "print(f'Variables: {variables}')\n",
                "print(f'Mean: {mean}')\n",
                "print(f'Std: {std}')\n",
                "\n",
                "# Count batches\n",
                "train_files, _ = get_batch_files('train')\n",
                "val_files, _ = get_batch_files('val')\n",
                "test_files, _ = get_batch_files('test')\n",
                "\n",
                "SAMPLES_PER_FILE = SAMPLES_PER_BATCH\n",
                "BATCHES_PER_FILE = (SAMPLES_PER_FILE + BATCH_SIZE - 1) // BATCH_SIZE\n",
                "n_train_batches = len(train_files) * BATCHES_PER_FILE\n",
                "n_val_batches = len(val_files) * BATCHES_PER_FILE\n",
                "n_test_batches = len(test_files) * BATCHES_PER_FILE\n",
                "\n",
                "print(f'\\nTrain: {len(train_files)} files = ~{n_train_batches} batches')\n",
                "print(f'Val: {len(val_files)} files = ~{n_val_batches} batches')\n",
                "print(f'Test: {len(test_files)} files = ~{n_test_batches} batches')\n",
                "\n",
                "# Quick benchmark\n",
                "print('\\nBenchmarking data loading...')\n",
                "t0 = time.time()\n",
                "for i, (x, y) in enumerate(batch_generator('train')):\n",
                "    if i >= 20:\n",
                "        break\n",
                "print(f'20 batches in {time.time()-t0:.2f}s ({(time.time()-t0)/20*1000:.1f}ms/batch)')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 6: Model Definition\n",
                "class ConvLSTMCell(nn.Module):\n",
                "    def __init__(self, input_dim, hidden_dim, kernel_size):\n",
                "        super().__init__()\n",
                "        self.hidden_dim = hidden_dim\n",
                "        self.conv = nn.Conv2d(input_dim + hidden_dim, 4 * hidden_dim,\n",
                "                              kernel_size, padding=kernel_size//2)\n",
                "    def forward(self, x, hidden):\n",
                "        h, c = hidden\n",
                "        gates = self.conv(torch.cat([x, h], dim=1))\n",
                "        i, f, o, g = torch.chunk(gates, 4, dim=1)\n",
                "        c_next = torch.sigmoid(f) * c + torch.sigmoid(i) * torch.tanh(g)\n",
                "        h_next = torch.sigmoid(o) * torch.tanh(c_next)\n",
                "        return h_next, c_next\n",
                "    def init_hidden(self, B, H, W, dev):\n",
                "        return (torch.zeros(B, self.hidden_dim, H, W, device=dev),\n",
                "                torch.zeros(B, self.hidden_dim, H, W, device=dev))\n",
                "\n",
                "class WeatherNowcaster(nn.Module):\n",
                "    def __init__(self, in_ch, hidden_dim, out_ch, n_layers=2):\n",
                "        super().__init__()\n",
                "        self.encoder = nn.ModuleList([ConvLSTMCell(in_ch if i==0 else hidden_dim, hidden_dim, 3) for i in range(n_layers)])\n",
                "        self.decoder = nn.ModuleList([ConvLSTMCell(out_ch if i==0 else hidden_dim, hidden_dim, 3) for i in range(n_layers)])\n",
                "        self.out_conv = nn.Conv2d(hidden_dim, out_ch, 1)\n",
                "    \n",
                "    def forward(self, x, future_steps):\n",
                "        B, T, C, H, W = x.shape\n",
                "        hidden = [cell.init_hidden(B, H, W, x.device) for cell in self.encoder]\n",
                "        for t in range(T):\n",
                "            inp = x[:, t]\n",
                "            for i, cell in enumerate(self.encoder):\n",
                "                h, c = cell(inp, hidden[i])\n",
                "                hidden[i] = (h, c)\n",
                "                inp = h\n",
                "        \n",
                "        dec_hidden = [(h.clone(), c.clone()) for h, c in hidden]\n",
                "        outputs = []\n",
                "        dec_in = self.out_conv(dec_hidden[-1][0])\n",
                "        for _ in range(future_steps):\n",
                "            for i, cell in enumerate(self.decoder):\n",
                "                h, c = cell(dec_in if i==0 else h, dec_hidden[i])\n",
                "                dec_hidden[i] = (h, c)\n",
                "            dec_in = self.out_conv(h)\n",
                "            outputs.append(dec_in)\n",
                "        return torch.stack(outputs, dim=1)\n",
                "\n",
                "HIDDEN_DIM = 128\n",
                "IN_CHANNELS = len(variables)\n",
                "OUT_CHANNELS = len(variables)\n",
                "\n",
                "model = WeatherNowcaster(IN_CHANNELS, HIDDEN_DIM, OUT_CHANNELS, n_layers=2).to(device)\n",
                "print(f'Model Parameters: {sum(p.numel() for p in model.parameters()):,}')\n",
                "\n",
                "# Benchmark forward pass\n",
                "print('\\nBenchmarking model...')\n",
                "for x, y in batch_generator('train'):\n",
                "    _, _, C, H, W = x.shape\n",
                "    break\n",
                "\n",
                "x_test = torch.randn(BATCH_SIZE, T_IN, IN_CHANNELS, H, W).to(device)\n",
                "torch.cuda.synchronize()\n",
                "t0 = time.time()\n",
                "for _ in range(10):\n",
                "    with autocast():\n",
                "        _ = model(x_test, T_OUT)\n",
                "    torch.cuda.synchronize()\n",
                "print(f'10 forward passes in {time.time()-t0:.2f}s ({(time.time()-t0)/10*1000:.1f}ms/batch)')\n",
                "del x_test\n",
                "torch.cuda.empty_cache()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell: Auto-Resume Logic & Drive Mounting\n",
                "import os\n",
                "import torch\n",
                "try:\n",
                "    from google.colab import drive\n",
                "    drive.mount('/content/drive')\n",
                "    print('\u2713 Google Drive mounted')\n",
                "    CHECKPOINT_DIR = '/content/drive/MyDrive/WeatherPaper/checkpoints'\n",
                "    os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
                "    print(f'\u2713 Checkpoints will be saved to: {CHECKPOINT_DIR}')\n",
                "except ImportError:\n",
                "    print('\u26a0\ufe0f Not running in Colab, using local paths')\n",
                "    CHECKPOINT_DIR = 'checkpoints'\n",
                "\n",
                "RESUME_FROM = None\n",
                "start_epoch = 0\n",
                "\n",
                "# Check for existing LAST checkpoint\n",
                "if os.path.exists(f'{CHECKPOINT_DIR}/last.pth'):\n",
                "    RESUME_FROM = f'{CHECKPOINT_DIR}/last.pth'\n",
                "elif os.path.exists(f'{CHECKPOINT_DIR}/best_model.pth'):\n",
                "    RESUME_FROM = f'{CHECKPOINT_DIR}/best_model.pth'\n",
                "\n",
                "if RESUME_FROM and os.path.exists(RESUME_FROM):\n",
                "    print(f'Loading checkpoint: {RESUME_FROM}')\n",
                "    checkpoint = torch.load(RESUME_FROM, map_location='cuda' if torch.cuda.is_available() else 'cpu')\n",
                "    if 'model' in checkpoint:\n",
                "        model.load_state_dict(checkpoint['model'])\n",
                "    else:\n",
                "        model.load_state_dict(checkpoint)\n",
                "    \n",
                "    if 'epoch' in checkpoint:\n",
                "        start_epoch = checkpoint['epoch'] + 1\n",
                "        print(f'Resuming from epoch {start_epoch}')\n",
                "    else:\n",
                "        print('Warning: Epoch info not found, fine-tuning from 0')\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 7: Training Loop with AMP\n",
                "criterion = nn.MSELoss()\n",
                "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
                "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)\n",
                "\n",
                "NUM_EPOCHS = 50\n",
                "best_val_loss = float('inf')\n",
                "train_losses, val_losses = [], []\n",
                "\n",
                "print(f'Training Configuration:')\n",
                "print(f'  Epochs: {NUM_EPOCHS}')\n",
                "print(f'  Batch size: {BATCH_SIZE}')\n",
                "print(f'  Hidden dim: {HIDDEN_DIM}')\n",
                "print(f'  Input: {T_IN}h \u2192 Output: {T_OUT}h')\n",
                "print('=' * 60)\n",
                "\n",
                "for epoch in range(start_epoch, NUM_EPOCHS):\n",
                "    epoch_start = time.time()\n",
                "    \n",
                "    model.train()\n",
                "    train_loss, n_batches = 0.0, 0\n",
                "    pbar = tqdm(batch_generator('train', BATCH_SIZE, shuffle=True), \n",
                "                total=n_train_batches, desc=f'Epoch {epoch+1} [Train]', leave=False)\n",
                "    \n",
                "    for x, y in pbar:\n",
                "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
                "        optimizer.zero_grad(set_to_none=True)\n",
                "        \n",
                "        with autocast():\n",
                "            out = model(x, T_OUT)\n",
                "            loss = criterion(out, y)\n",
                "        \n",
                "        scaler.scale(loss).backward()\n",
                "        scaler.unscale_(optimizer)\n",
                "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
                "        scaler.step(optimizer)\n",
                "        scaler.update()\n",
                "        \n",
                "        train_loss += loss.item()\n",
                "        n_batches += 1\n",
                "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
                "    \n",
                "    train_loss /= n_batches\n",
                "    train_losses.append(train_loss)\n",
                "    \n",
                "    model.eval()\n",
                "    val_loss, n_val = 0.0, 0\n",
                "    with torch.no_grad():\n",
                "        for x, y in tqdm(batch_generator('val', BATCH_SIZE), total=n_val_batches, \n",
                "                         desc=f'Epoch {epoch+1} [Val]', leave=False):\n",
                "            x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
                "            with autocast():\n",
                "                val_loss += criterion(model(x, T_OUT), y).item()\n",
                "            n_val += 1\n",
                "    \n",
                "    val_loss /= n_val\n",
                "    val_losses.append(val_loss)\n",
                "    scheduler.step(val_loss)\n",
                "    \n",
                "    epoch_time = time.time() - epoch_start\n",
                "    marker = '\u2605 BEST' if val_loss < best_val_loss else ''\n",
                "    print(f'Epoch {epoch+1:2d} | Train: {train_loss:.6f} | Val: {val_loss:.6f} | Time: {epoch_time:.1f}s {marker}')\n",
                "    \n",
                "    if val_loss < best_val_loss:\n",
                "        best_val_loss = val_loss\n",
                "        torch.save({'model': model.state_dict(), 'val_loss': val_loss}, \n",
                "                   f'{CHECKPOINT_DIR}/best_model.pth')\n",
                "    \n",
                "    if (epoch + 1) % 10 == 0:\n",
                "        torch.save({'epoch': epoch, 'model': model.state_dict(),\n",
                "                    'train_losses': train_losses, 'val_losses': val_losses},\n",
                "                   f'{CHECKPOINT_DIR}/checkpoint_e{epoch+1}.pth')\n",
                "    \n",
                "    gc.collect()\n",
                "    torch.cuda.empty_cache()\n",
                "\n",
                "print('=' * 60)\n",
                "print(f'\u2713 Training complete! Best val loss: {best_val_loss:.6f}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 8: Plot Training Curves\n",
                "plt.figure(figsize=(10, 5))\n",
                "plt.plot(train_losses, label='Train', linewidth=2)\n",
                "plt.plot(val_losses, label='Val', linewidth=2)\n",
                "plt.xlabel('Epoch')\n",
                "plt.ylabel('MSE Loss')\n",
                "plt.title('Training Progress')\n",
                "plt.legend()\n",
                "plt.grid(alpha=0.3)\n",
                "plt.savefig(f'{FIGURES_DIR}/training_curve.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "print(f'Best val loss: {best_val_loss:.6f}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 9: Sample Prediction\n",
                "model.eval()\n",
                "for x, y in batch_generator('val'):\n",
                "    x_sample = x[:1].to(device)\n",
                "    y_sample = y[:1]\n",
                "    break\n",
                "\n",
                "with torch.no_grad():\n",
                "    with autocast():\n",
                "        pred = model(x_sample, T_OUT)\n",
                "\n",
                "pred_np = pred[0].cpu().float().numpy()\n",
                "true_np = y_sample[0].numpy()\n",
                "\n",
                "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
                "for i, t in enumerate([0, 2, 5]):\n",
                "    axes[0, i].imshow(true_np[t, 0], cmap='Blues')\n",
                "    axes[0, i].set_title(f'Truth t+{t+1}h')\n",
                "    axes[1, i].imshow(pred_np[t, 0], cmap='Blues')\n",
                "    axes[1, i].set_title(f'Pred t+{t+1}h')\n",
                "plt.suptitle('Precipitation Forecast', fontsize=14)\n",
                "plt.tight_layout()\n",
                "plt.savefig(f'{FIGURES_DIR}/sample_prediction.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 10: Save Final Model\n",
                "torch.save({\n",
                "    'model_state_dict': model.state_dict(),\n",
                "    'config': {'hidden_dim': HIDDEN_DIM, 'n_layers': 2, 'T_IN': T_IN, 'T_OUT': T_OUT},\n",
                "    'mean': mean, 'std': std, 'variables': variables,\n",
                "    'best_val_loss': best_val_loss,\n",
                "    'train_losses': train_losses, 'val_losses': val_losses\n",
                "}, f'{CHECKPOINT_DIR}/final_model.pth')\n",
                "\n",
                "print(f'\u2713 Saved to: {CHECKPOINT_DIR}/final_model.pth')\n",
                "print(f'Best validation loss: {best_val_loss:.6f}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 11: Test on 2025 Data\n",
                "print('Evaluating on test set (includes 2025 data)...')\n",
                "model.eval()\n",
                "test_loss, n_test = 0.0, 0\n",
                "\n",
                "with torch.no_grad():\n",
                "    for x, y in tqdm(batch_generator('test', BATCH_SIZE), total=n_test_batches, desc='Testing'):\n",
                "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
                "        with autocast():\n",
                "            test_loss += criterion(model(x, T_OUT), y).item()\n",
                "        n_test += 1\n",
                "\n",
                "test_loss /= n_test if n_test > 0 else 1\n",
                "print(f'\\n\u2713 Test Loss (2024-2025 data): {test_loss:.6f}')"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4"
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}